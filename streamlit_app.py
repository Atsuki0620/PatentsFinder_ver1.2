import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage

# ------------------------------------------------------------
# 0. SessionState ã®åˆæœŸåŒ–
# ------------------------------------------------------------
if "phase" not in st.session_state:
    st.session_state["phase"] = "api_key_input"

if "openai_key" not in st.session_state:
    st.session_state["openai_key"] = ""

if "user_inputs" not in st.session_state:
    st.session_state["user_inputs"] = []

if "latest_plan" not in st.session_state:
    st.session_state["latest_plan"] = ""

if "json_params" not in st.session_state:
    st.session_state["json_params"] = ""

if "greeted" not in st.session_state:
    st.session_state["greeted"] = False

if "plan_shown" not in st.session_state:
    st.session_state["plan_shown"] = False

if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []


# ------------------------------------------------------------
# 1. Chat å±¥æ­´ã‚’ç”»é¢ã«æç”»ã™ã‚‹é–¢æ•°
# ------------------------------------------------------------
def render_chat_history():
    for msg in st.session_state["chat_history"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


# ------------------------------------------------------------
# 2. ãƒ•ã‚§ãƒ¼ã‚ºâ‘ ï¼šAPIã‚­ãƒ¼å…¥åŠ›ç”¨ UIï¼ˆä¿®æ­£ç‰ˆï¼‰
# ------------------------------------------------------------
def phase_api_key_input():
    st.title("ğŸ’¬ PatentsFinder_ver1.2")
    st.write(
        "ã“ã®ã‚¢ãƒ—ãƒªã¯ç‰¹è¨±èª¿æŸ»ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n"
        "æœ€åˆã« OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã€ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"
    )

    # ã™ã§ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã« API ã‚­ãƒ¼ãŒã‚ã‚Œã°æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºã¸
    if st.session_state["openai_key"]:
        st.session_state["phase"] = "awaiting_query"
        return

    # ã¾ã ã‚­ãƒ¼ãŒç©ºã®ã¨ãã ã‘ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨é€ä¿¡ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
    key_input = st.text_input(
        "OpenAI API Key", 
        type="password", 
        value=st.session_state["openai_key"]
    )

    # ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‚‰ãƒ•ã‚§ãƒ¼ã‚ºé·ç§»
    if st.button("é€ä¿¡"):
        if key_input:
            st.session_state["openai_key"] = key_input
            st.session_state["phase"] = "awaiting_query"
            st.session_state["greeted"] = False
            return
        else:
            st.error("API ã‚­ãƒ¼ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†åº¦ã”ç¢ºèªãã ã•ã„ã€‚")


# ------------------------------------------------------------
# 3. ãƒ•ã‚§ãƒ¼ã‚ºâ‘¡ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼èª¿æŸ»ãƒ‹ãƒ¼ã‚ºãƒ’ã‚¢ãƒªãƒ³ã‚°
# ------------------------------------------------------------
def phase_awaiting_query():
    if not st.session_state["greeted"]:
        greeting = (
            "ã“ã‚“ã«ã¡ã¯ï¼ã©ã‚“ãªç‰¹è¨±æƒ…å ±ã‚’ãŠæ¢ã—ã§ã™ã‹ï¼Ÿ\n"
            "æ°—ã«ãªã‚‹æŠ€è¡“ã‚„å…¬é–‹æ—¥ã€èª¿æŸ»ã—ãŸã„å›½ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã©ã€æ€ã„ã¤ãã¾ã¾æ•™ãˆã¦ãã ã•ã„ï¼"
        )
        st.session_state["chat_history"].append({"role": "assistant", "content": greeting})
        st.session_state["greeted"] = True

    render_chat_history()

    if user_query := st.chat_input("èª¿æŸ»ãƒ‹ãƒ¼ã‚ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„â€¦"):
        st.session_state["chat_history"].append({"role": "user", "content": user_query})
        st.session_state["user_inputs"].append(user_query)
        st.session_state["phase"] = "proposed_plan"


# ------------------------------------------------------------
# 4. ãƒ•ã‚§ãƒ¼ã‚ºâ‘¢ï¼šæ¤œç´¢æ–¹é‡ææ¡ˆï¼ˆè‡ªç„¶è¨€èªï¼‰
# ------------------------------------------------------------
def phase_proposed_plan():
    if not st.session_state["plan_shown"]:
        render_chat_history()

        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            openai_api_key=st.session_state["openai_key"],
            temperature=0.7,
            streaming=False,
        )

        prompt_lines = []
        for idx, txt in enumerate(st.session_state["user_inputs"], start=1):
            prompt_lines.append(f"{idx}. {txt}")
        joined_input = "\n".join(prompt_lines)

        system_prompt = (
            "ã‚ãªãŸã¯ç‰¹è¨±èª¿æŸ»æ”¯æ´ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã‚’ã‚‚ã¨ã«ã€ŒæŠ€è¡“åˆ†é‡ã€ã€Œå…¬é–‹æ—¥ã€ã€Œå‡ºé¡˜å›½ã€ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€"
            "ã‚’å«ã‚€æ¤œç´¢æ–¹é‡ã‚’è‡ªç„¶è¨€èªã§1ï½2æ–‡ç¨‹åº¦ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        human_prompt = (
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®èª¿æŸ»ãƒ‹ãƒ¼ã‚º:\n"
            f"{joined_input}\n\n"
            "ä¸Šè¨˜ã‚’è¸ã¾ãˆã¦ã€ç°¡æ½”ã«æ¤œç´¢æ–¹é‡ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚"
        )

        messages = [
            HumanMessage(content=system_prompt),
            HumanMessage(content=human_prompt),
        ]

        response = llm(messages)
        plan_text = response.content.strip()

        st.session_state["latest_plan"] = plan_text
        st.session_state["chat_history"].append({"role": "assistant", "content": plan_text})
        st.session_state["plan_shown"] = True

        render_chat_history()
    else:
        render_chat_history()

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ã“ã®æ–¹é‡ã§æ¤œç´¢å®Ÿè¡Œ"):
            st.session_state["phase"] = "generate_json"
    with col2:
        if st.button("æ–¹é‡ã‚’ä¿®æ­£ã™ã‚‹"):
            st.session_state["plan_shown"] = False
            st.session_state["phase"] = "awaiting_query"


# ------------------------------------------------------------
# 5. ãƒ•ã‚§ãƒ¼ã‚ºâ‘£ï¼šJSONå½¢å¼ã®æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆ
# ------------------------------------------------------------
def phase_generate_json():
    render_chat_history()

    if not st.session_state["json_params"]:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            openai_api_key=st.session_state["openai_key"],
            temperature=0.0,
            streaming=False,
        )

        natural_plan = st.session_state["latest_plan"]
        system_prompt = (
            "ã‚ãªãŸã¯ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢æ”¯æ´ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "ä»¥ä¸‹ã®è‡ªç„¶è¨€èªã®æ¤œç´¢æ–¹é‡ã‚’å³å¯†ã« JSON ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        )
        human_prompt = (
            f"è‡ªç„¶è¨€èªã®æ¤œç´¢æ–¹é‡:\nã€Œ{natural_plan}ã€\n\n"
            "ã“ã®æ–¹é‡ã‚’ã‚‚ã¨ã«ã€ä»¥ä¸‹ã®å½¢å¼ã® JSON ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\n"
            "```\n"
            "{\n"
            '  "technical_field": "<æŠ€è¡“åˆ†é‡>",\n'
            '  "public_date_from": "YYYY-MM-DD",\n'
            '  "public_date_to": "YYYY-MM-DD",\n'
            '  "countries": ["<å›½ã‚³ãƒ¼ãƒ‰1>", "<å›½ã‚³ãƒ¼ãƒ‰2>", ...],\n'
            '  "keywords": ["<ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1>", "<ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2>", ...]\n'
            "}\n"
            "```\n"
            "â€» ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¯å³å¯†ã«ä¸Šè¨˜ã®åå‰ã‚’ä½¿ã„ã€ä»–ã®ã‚­ãƒ¼ã‚’è¿½åŠ ã—ãªã„ã§ãã ã•ã„ã€‚\n"
            "â€» å‡ºåŠ›ã¯ JSON ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã ã‘ã¨ã—ã€ä½™è¨ˆãªèª¬æ˜ã‚„æ–‡ç« ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚"
        )

        messages = [
            HumanMessage(content=system_prompt),
            HumanMessage(content=human_prompt),
        ]

        response = llm(messages)
        json_text = response.content.strip()

        st.session_state["json_params"] = json_text
        st.session_state["chat_history"].append({"role": "assistant", "content": "æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJSONå½¢å¼ï¼‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"})
        render_chat_history()

    st.markdown("**â–¼ ä»¥ä¸‹ãŒç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJSONå½¢å¼ï¼‰ã§ã™ â–¼**")
    st.code(st.session_state["json_params"], language="json")


# ------------------------------------------------------------
# 6. main é–¢æ•°ç›¸å½“ï¼šfase ã«ã‚ˆã£ã¦å‡¦ç†ã‚’æŒ¯ã‚Šåˆ†ã‘
# ------------------------------------------------------------
def main():
    if st.session_state["phase"] == "api_key_input":
        phase_api_key_input()
    elif st.session_state["phase"] == "awaiting_query":
        phase_awaiting_query()
    elif st.session_state["phase"] == "proposed_plan":
        phase_proposed_plan()
    elif st.session_state["phase"] == "generate_json":
        phase_generate_json()
    else:
        st.session_state["phase"] = "api_key_input"
        phase_api_key_input()


if __name__ == "__main__":
    main()
